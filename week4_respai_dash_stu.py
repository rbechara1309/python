# -*- coding: utf-8 -*-
"""Week4_RespAI_Dash_STU.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dENu9pxJf7MfOFTxTWyJ_8r8QF7dVU29
"""

# student_dashboard.py
"""
Responsible AI Dashboard Starter Code (Student Lab)
Objective: Build a dashboard that shows model performance and fairness.
Instructions:
- Fill in the missing parts marked with TODO
- You will train a simple Decision Tree classifier, compute metrics,
  and display results in a Streamlit dashboard.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import LabelEncoder
import streamlit as st
import matplotlib.pyplot as plt

# ---------------------
# Part 1: Load Dataset
# ---------------------
# TODO: Load the Adult dataset from UCI
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
columns = ["age","workclass","fnlwgt","education","education-num","marital-status",
           "occupation","relationship","race","sex","capital-gain","capital-loss",
           "hours-per-week","native-country","income"]

# TODO: Read CSV into pandas dataframe and drop missing values
# ---------------------
# Part 1: Read Data
# ---------------------
# Segundo o Passo 1 da apostila, o tratamento inicial envolve remover dados inválidos[cite: 47, 48].
data = pd.read_csv(url, names=columns, na_values="?", skipinitialspace=True)
data = data.dropna() # Removendo valores ausentes conforme orientado[cite: 48].

# ---------------------
# Part 2: Preprocess
# ---------------------
# TODO: Encode categorical columns using LabelEncoder
# Example: for col in data.select_dtypes(include="object"): ...
# Padronização de saídas esperadas como valores numéricos
le = LabelEncoder()
for col in data.select_dtypes(include="object").columns:
    data[col] = le.fit_transform(data[col])

# ---------------------
# Part 3: Train/Test Split
# ---------------------
# O dataset é dividido normalmente entre 70% a 80% para treino e o restante para teste.
X = data.drop('income', axis=1) # features
y = data['income'] # labels (rótulos)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ---------------------
# Part 4: Train Model
# ---------------------
# TODO: Train a DecisionTreeClassifier on the training set
# A escolha do modelo (Decision Tree) ocorre de acordo com o objetivo de classificação.
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# ---------------------
# Part 5: Evaluate Model
# ---------------------
# No Passo 4, realizamos o teste para obter o comportamento real do modelo[cite: 56].
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)

# ---------------------
# Part 6: Fairness by Group
# ---------------------
def group_accuracy(model, X, y, group_attr):
    results = {}
    # Calculando a eficiência para cada grupo único do atributo protegido[cite: 56, 283].
    unique_groups = X[group_attr].unique()
    for group in unique_groups:
        mask = (X[group_attr] == group)
        group_acc = accuracy_score(y[mask], model.predict(X[mask]))
        results[group] = group_acc
    return results

fairness_results = group_accuracy(model, X_test, y_test, "sex")

# ---------------------
# Part 7: Streamlit Dashboard
# ---------------------
st.title("Responsible AI Dashboard (Student Lab)")

# Exibindo métricas de performance conforme o workflow[cite: 283].
st.subheader("Model Performance Metrics")
col1, col2, col3 = st.columns(3)
col1.metric("Accuracy", f"{accuracy:.2f}")
col2.metric("Precision", f"{precision:.2f}")
col3.metric("Recall", f"{recall:.2f}")

# Gráfico de equidade
st.subheader("Fairness by Group (Protected Attribute)")
fig, ax = plt.subplots()
ax.bar(fairness_results.keys(), fairness_results.values())
ax.set_ylabel("Accuracy")
ax.set_ylim(0, 1)
st.pyplot(fig)

# Reflexão sobre IA Responsável
st.subheader("Responsible AI Reflection")
st.write("""
Monitoring model metrics and fairness is fundamental for transparency and accountability.
By documenting 'Prediction vs. Reality,' we ensure the model's efficiency is measurable
and its impact is equitable across different groups. This prevents 'black box'
decision-making and allows for iterative improvements in data preparation to
mitigate algorithmic bias.
""")

# ---------------------
# Visualização no Colab
# ---------------------
print("\n--- Fairness by Group (Protected Attribute) ---")
plt.figure(figsize=(8, 5))
plt.bar(fairness_results.keys(), fairness_results.values(), color=['blue', 'pink'])
plt.ylabel("Accuracy")
plt.title("Model Fairness by Sex")
plt.ylim(0, 1)
plt.show() # Este comando renderiza o gráfico na célula do Colab.

print("\n--- Responsible AI Reflection ---")
print("Monitoring model metrics and fairness is fundamental for transparency and accountability. ")
print("By documenting 'Prediction vs. Reality,' we ensure the model's efficiency is measurable ")
print("and its impact is equitable across different groups. This prevents 'black box' ")
print("decision-making and allows for iterative improvements in data preparation to ")
print("mitigate algorithmic bias.")

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

# Realizando as previsões com os dados de teste
y_pred = model.predict(X_test)

# Calculando as métricas de eficiência
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)

print("--- RESULTADOS DO TREINAMENTO (PASSO 4) ---")
print(f"Accuracy (Acurácia):  {acc:.4f}")
print(f"Precision (Precisão): {prec:.4f}")
print(f"Recall (Revocação):   {rec:.4f}")

# Exibindo a matriz de confusão numérica
print("\nMatriz de Confusão:")
print(confusion_matrix(y_test, y_pred))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# 1. Calcular a matriz numérica
# Substitua 'y_test' e 'y_pred' pelas suas variáveis de teste e previsão
cm = confusion_matrix(y_test, y_pred)

# 2. Configurar o visual com Seaborn (Heatmap)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Não Selecionado', 'Selecionado'],
            yticklabels=['Não Selecionado', 'Selecionado'])

# 3. Adicionar rótulos detalhados para o Passo 4 (Avaliação)
plt.title('Matriz de Confusão: Comparação Realidade vs. Previsão', fontsize=14)
plt.ylabel('Valor Real (Ground Truth)', fontsize=12)
plt.xlabel('Valor Previsto pelo Modelo', fontsize=12)

# Exibir o gráfico no Colab ou salvar
plt.show()
